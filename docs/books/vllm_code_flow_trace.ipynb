{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277104ee",
   "metadata": {},
   "source": [
    "# vLLM: OpenAI REST API to LLM Inference Code Flow Trace\n",
    "\n",
    "This notebook provides a comprehensive trace of the code flow from OpenAI REST API calls to actual LLM inference in vLLM v0.10.0.\n",
    "\n",
    "## Overview\n",
    "This document traces the complete journey of an API request through the vLLM codebase, including:\n",
    "- API endpoint handling\n",
    "- Request preprocessing and validation\n",
    "- Engine scheduling\n",
    "- Model execution\n",
    "- Output post-processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd76b1",
   "metadata": {},
   "source": [
    "## Layer 1: OpenAI REST API Entry Point\n",
    "\n",
    "### File: `vllm/entrypoints/openai/api_server.py`\n",
    "\n",
    "When a client makes a request to the OpenAI-compatible API endpoint (e.g., `/v1/chat/completions`), the request is handled by FastAPI routes defined in this file.\n",
    "\n",
    "**Key Classes & Functions:**\n",
    "- `ChatCompletionRequest` - Request protocol/schema\n",
    "- `create_chat_completion()` - Main endpoint handler for chat completions\n",
    "- `app` - FastAPI application instance\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "HTTP Request\n",
    "    ↓\n",
    "FastAPI Route Handler (e.g., @app.post(\"/v1/chat/completions\"))\n",
    "    ↓\n",
    "parse_request() / validate_request()\n",
    "    ↓\n",
    "create_chat_completion(request: ChatCompletionRequest)\n",
    "```\n",
    "\n",
    "**Code Reference:**\n",
    "```python\n",
    "# vllm/entrypoints/openai/api_server.py:1857 lines\n",
    "# Contains OpenAI-compatible API server implementation\n",
    "# Uses FastAPI for HTTP handling\n",
    "# Routes requests to appropriate serving handlers\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd6c87",
   "metadata": {},
   "source": [
    "## Layer 2: Request Serving & Processing\n",
    "\n",
    "### File: `vllm/entrypoints/openai/serving_chat.py`\n",
    "\n",
    "The `OpenAIServingChat` class handles chat-specific request processing.\n",
    "\n",
    "**Key Classes:**\n",
    "- `OpenAIServingChat(OpenAIServing)` - Handles chat completion logic\n",
    "- `OpenAIServing` - Base serving class with common functionality\n",
    "\n",
    "**Main Method: `create_chat_completion()`**\n",
    "\n",
    "Steps:\n",
    "1. **Check model validity** - Verify model exists and is loaded\n",
    "2. **Get tokenizer** - Retrieve the tokenizer for this request\n",
    "3. **Preprocess chat messages** - Convert chat messages to prompt format using chat template\n",
    "4. **Create sampling parameters** - Set generation parameters (temperature, top_p, etc.)\n",
    "5. **Prepare engine prompts** - Convert preprocessed messages to format engine expects\n",
    "6. **Generate completion** - Call engine to generate tokens\n",
    "\n",
    "**Code Flow:**\n",
    "```python\n",
    "create_chat_completion(request: ChatCompletionRequest)\n",
    "    ├── _check_model(request)\n",
    "    ├── engine_client.get_tokenizer(lora_request)\n",
    "    ├── _preprocess_chat(\n",
    "    │   ├── messages → conversation\n",
    "    │   ├── chat_template application\n",
    "    │   └── return: (conversation, request_prompts, engine_prompts)\n",
    "    ├── Create SamplingParams from request\n",
    "    └── _generate_completions(engine_prompts, sampling_params)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95964a1",
   "metadata": {},
   "source": [
    "## Layer 3: Engine Interface (Async Layer)\n",
    "\n",
    "### File: `vllm/entrypoints/openai/serving_engine.py`\n",
    "\n",
    "**Key Class: `OpenAIServing`**\n",
    "\n",
    "This is the base class handling:\n",
    "- Request validation\n",
    "- Input/output format conversion\n",
    "- Engine client communication\n",
    "\n",
    "**Critical Methods:**\n",
    "```python\n",
    "def _generate_completions(\n",
    "    engine_prompts, \n",
    "    sampling_params, \n",
    "    request_id\n",
    ")\n",
    "    ├── engine_client.add_request(request_id, prompt, sampling_params)\n",
    "    └── engine_client.generate(request_id)  # Returns async generator\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "1. Request ID is generated (unique identifier for this request)\n",
    "2. Prompts are passed to the engine\n",
    "3. Sampling parameters are attached\n",
    "4. Engine returns an async generator of results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531ff7f",
   "metadata": {},
   "source": [
    "## Layer 4: Async LLM Engine\n",
    "\n",
    "### File: `vllm/engine/async_llm_engine.py`\n",
    "\n",
    "**Key Class: `AsyncLLMEngine`**\n",
    "\n",
    "This wraps the synchronous `LLMEngine` for async operation.\n",
    "\n",
    "**Key Methods:**\n",
    "```python\n",
    "async def add_request(\n",
    "    request_id: str,\n",
    "    prompt: PromptType,\n",
    "    sampling_params: SamplingParams,\n",
    "    lora_request: Optional[LoRARequest] = None\n",
    ")\n",
    "    ├── RequestTracker.add_request() - Track new request\n",
    "    └── _request_streams[request_id] = AsyncStream()  # Create result stream\n",
    "\n",
    "async def generate(request_id: str)\n",
    "    ├── while request not finished:\n",
    "    ├── yield await request_stream.get_next_output()\n",
    "    └── return final output\n",
    "```\n",
    "\n",
    "**Request Tracking:**\n",
    "- `RequestTracker` maintains queues of new/pending requests\n",
    "- `AsyncStream` provides an async generator for each request\n",
    "- Engine loop processes requests in background\n",
    "\n",
    "**Important: The actual scheduling & execution happens in the background engine loop:**\n",
    "```python\n",
    "async def run_engine_loop()  # Background task\n",
    "    ├── while True:\n",
    "    ├── process_new_requests()  # From RequestTracker\n",
    "    ├── step()  # Call engine.step()\n",
    "    └── update_request_streams()  # Send outputs to streams\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199fcb1",
   "metadata": {},
   "source": [
    "## Layer 5: Core LLM Engine (Synchronous)\n",
    "\n",
    "### File: `vllm/engine/llm_engine.py`\n",
    "\n",
    "**Key Class: `LLMEngine`**\n",
    "\n",
    "This is the core engine orchestrating the entire inference pipeline.\n",
    "\n",
    "### Phase 1: Request Addition\n",
    "\n",
    "**Method: `add_request()`** (Lines 619-769)\n",
    "\n",
    "```python\n",
    "def add_request(\n",
    "    request_id: str,\n",
    "    prompt: PromptType,\n",
    "    params: Union[SamplingParams, PoolingParams],\n",
    "    arrival_time: Optional[float] = None,\n",
    "    lora_request: Optional[LoRARequest] = None,\n",
    "    ...\n",
    ")\n",
    "    ├── Input validation\n",
    "    ├── Preprocess prompt\n",
    "    │   └── input_preprocessor.preprocess()\n",
    "    ├── Tokenize prompt (if needed)\n",
    "    ├── Create Sequence objects\n",
    "    │   └── One per output (n=sampling_params.n)\n",
    "    ├── Create SequenceGroup (batching unit)\n",
    "    └── Add to scheduler\n",
    "        └── self.scheduler.add_seq_group()\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "- Prompt is converted to token IDs via tokenizer\n",
    "- `Sequence` objects track individual output sequences\n",
    "- `SequenceGroup` represents all outputs for a single request\n",
    "- Request goes into the scheduler's waiting queue\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019dcbd6",
   "metadata": {},
   "source": [
    "## Phase 2: Scheduling & Model Execution\n",
    "\n",
    "### Method: `step()` (Lines 1194-1494)\n",
    "\n",
    "This is called repeatedly in a loop and is the **core inference iteration**.\n",
    "\n",
    "**Overview:**\n",
    "```python\n",
    "def step() -> List[Union[RequestOutput, PoolingRequestOutput]]:\n",
    "    # STEP 1: Schedule\n",
    "    ├── scheduler.schedule()  # Decide which sequences to run next\n",
    "    │   ├── Select sequences from pending queue\n",
    "    │   ├── Determine KV cache operations (swap in/out/copy)\n",
    "    │   ├── Return: seq_group_metadata_list, scheduler_outputs\n",
    "    │   \n",
    "    # STEP 2: Execute Model\n",
    "    ├── model_executor.execute_model(execute_model_req)\n",
    "    │   └── Returns: List[SamplerOutput]\n",
    "    │\n",
    "    # STEP 3: Post-Process Outputs\n",
    "    ├── _process_model_outputs()\n",
    "    │   ├── Decode tokens to text\n",
    "    │   ├── Update sequences with new tokens\n",
    "    │   ├── Check stop conditions\n",
    "    │   ├── Sample next tokens\n",
    "    │   └── Mark finished sequences\n",
    "    │\n",
    "    # STEP 4: Return Results\n",
    "    └── return request_outputs\n",
    "```\n",
    "\n",
    "**Key Variables:**\n",
    "- `seq_group_metadata_list` - Metadata about sequences being executed\n",
    "- `scheduler_outputs` - Blocks to swap, cache operations\n",
    "- `execute_model_req` - Request passed to executor\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22366dd",
   "metadata": {},
   "source": [
    "## Layer 6: Scheduler\n",
    "\n",
    "### File: `vllm/core/scheduler.py`\n",
    "\n",
    "**Key Class: `Scheduler`**\n",
    "\n",
    "**Method: `schedule()` returns `SchedulerOutputs`**\n",
    "\n",
    "**What it does:**\n",
    "```\n",
    "Scheduler Input:\n",
    "├── Pending requests queue\n",
    "├── Running sequences (need more tokens)\n",
    "└── KV cache status\n",
    "\n",
    "Process:\n",
    "├── Priority-based selection\n",
    "├── Determine:\n",
    "│   ├── Which sequences to run (fit in GPU memory)\n",
    "│   ├── Which KV cache blocks to swap in/out\n",
    "│   ├── Which blocks to copy (for beam search)\n",
    "│   └── Prefill vs decode phase decision\n",
    "└── Build seq_group_metadata_list\n",
    "\n",
    "Output (SchedulerOutputs):\n",
    "├── scheduled_seq_groups - Sequences to run\n",
    "├── blocks_to_swap_in - KV cache blocks to load from CPU\n",
    "├── blocks_to_swap_out - KV cache blocks to save to CPU\n",
    "├── blocks_to_copy - KV cache blocks to duplicate\n",
    "└── num_lookahead_slots - For speculative decoding\n",
    "```\n",
    "\n",
    "**Key Decision:** The scheduler packs as many sequences as possible into the batch while respecting:\n",
    "- Maximum batch size\n",
    "- GPU memory constraints\n",
    "- KV cache limits\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226442b5",
   "metadata": {},
   "source": [
    "## Layer 7: Model Executor\n",
    "\n",
    "### File: `vllm/executor/executor_base.py` (Base) & `vllm/executor/uniproc_executor.py` (Single GPU)\n",
    "\n",
    "**Key Class: `ExecutorBase`** and implementations like `UniProcExecutor`\n",
    "\n",
    "**Method: `execute_model(execute_model_req: ExecuteModelRequest)`**\n",
    "\n",
    "For single-GPU setup:\n",
    "```python\n",
    "def execute_model(execute_model_req: ExecuteModelRequest):\n",
    "    ├── Call driver_worker.execute_model(execute_model_req)\n",
    "    └── return driver_outputs  # List[SamplerOutput]\n",
    "```\n",
    "\n",
    "For distributed setups:\n",
    "```python\n",
    "def execute_model(execute_model_req):\n",
    "    ├── Start worker execution loop (if not running)\n",
    "    ├── Execute model in driver worker\n",
    "    ├── Broadcast metadata to other workers\n",
    "    ├── Wait for all workers to complete\n",
    "    └── Return outputs from driver\n",
    "```\n",
    "\n",
    "**What ExecuteModelRequest contains:**\n",
    "```python\n",
    "ExecuteModelRequest:\n",
    "├── seq_group_metadata_list - Metadata for each sequence\n",
    "├── blocks_to_swap_in/out/copy - KV cache operations\n",
    "├── num_lookahead_slots - For speculative decoding\n",
    "└── async_callback - For async output processing\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c9170",
   "metadata": {},
   "source": [
    "## Layer 8: Worker\n",
    "\n",
    "### File: `vllm/worker/worker_base.py`\n",
    "\n",
    "**Key Class: `LocalOrDistributedWorkerBase`**\n",
    "\n",
    "**Method: `execute_model(execute_model_req: ExecuteModelRequest)`**\n",
    "\n",
    "```python\n",
    "def execute_model(execute_model_req):\n",
    "    ├── If distributed: Broadcast seq_group_metadata to other workers\n",
    "    ├── prepare_worker_input(execute_model_req)\n",
    "    │   └── Convert to WorkerInput format\n",
    "    ├── Handle KV cache operations\n",
    "    │   ├── Swap in blocks from CPU\n",
    "    │   ├── Swap out blocks to CPU\n",
    "    │   └── Copy blocks (for beam search)\n",
    "    ├── execute_worker(worker_input)\n",
    "    │   └── Calls model_runner.execute_model()\n",
    "    └── return outputs\n",
    "```\n",
    "\n",
    "**What happens in worker:**\n",
    "1. Prepare input tensors from metadata\n",
    "2. Load KV cache if needed (from CPU swap space)\n",
    "3. Call the model runner's execute_model\n",
    "4. Save outputs and KV cache\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601b0d4",
   "metadata": {},
   "source": [
    "## Layer 9: Model Runner\n",
    "\n",
    "### File: `vllm/worker/model_runner.py`\n",
    "\n",
    "**Key Class: `ModelRunner`** (inherits from `ModelRunnerBase`)\n",
    "\n",
    "**Method: `execute_model(model_input, kv_caches, ...)`** (Line 234+)\n",
    "\n",
    "This is where the actual neural network forward pass happens!\n",
    "\n",
    "```python\n",
    "def execute_model(model_input, kv_caches, num_steps=1):\n",
    "    ├── prepare_inputs_for_generation()\n",
    "    │   ├── _prepare_model_input_tensors()\n",
    "    │   │   ├── Prepare input_ids (token IDs)\n",
    "    │   │   ├── Create attention masks\n",
    "    │   │   ├── Position IDs\n",
    "    │   │   ├── Multi-modal data (if applicable)\n",
    "    │   │   └── Any LoRA configuration\n",
    "    │   │\n",
    "    │   └── model_input = ModelInputForGeneration()\n",
    "    │\n",
    "    ├── Apply guided decoding logits processor\n",
    "    ├── Apply custom logits processors\n",
    "    │\n",
    "    ├── forward() - **NEURAL NETWORK INFERENCE**\n",
    "    │   ├── model.forward(\n",
    "    │   │   input_ids,\n",
    "    │   │   position_ids,\n",
    "    │   │   attention_mask,\n",
    "    │   │   kv_caches,\n",
    "    │   │   ...\n",
    "    │   │ )\n",
    "    │   └── output = ModelOutput (logits, cache)\n",
    "    │\n",
    "    ├── Sampler - Token selection\n",
    "    │   ├── Apply temperature scaling\n",
    "    │   ├── Apply top-k filtering\n",
    "    │   ├── Apply top-p (nucleus) sampling\n",
    "    │   ├── Apply logits processors\n",
    "    │   ├── Sample next token\n",
    "    │   └── output = SamplerOutput\n",
    "    │\n",
    "    └── return SamplerOutput (logits, sampled_tokens, etc.)\n",
    "```\n",
    "\n",
    "**Critical Variables:**\n",
    "- `input_ids` - Token IDs for current batch\n",
    "- `position_ids` - Position in sequence\n",
    "- `attention_mask` - Which tokens to attend to\n",
    "- `kv_caches` - Cached key-value tensors for efficiency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a66f15",
   "metadata": {},
   "source": [
    "## Layer 10: LLM Model Forward Pass\n",
    "\n",
    "### File: Depends on model architecture (e.g., `vllm/model_executor/models/llama.py`)\n",
    "\n",
    "**This is where the actual transformer model lives!**\n",
    "\n",
    "```python\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids: torch.Tensor,      # Shape: [batch_size, seq_len]\n",
    "    positions: torch.Tensor,      # Positions in sequences\n",
    "    attn_metadata,                # Attention metadata\n",
    "    past_key_values,              # KV cache (for efficiency)\n",
    "    ...\n",
    ") -> torch.Tensor:               # Shape: [batch_size, seq_len, vocab_size]\n",
    "    \n",
    "    ├── Embed tokens\n",
    "    │   └── output = embedding(input_ids)\n",
    "    │\n",
    "    ├── For each transformer block:\n",
    "    │   ├── Self-attention with KV cache\n",
    "    │   │   ├── Query = input @ W_q\n",
    "    │   │   ├── Key/Value from cache or compute\n",
    "    │   │   ├── Attention weights = softmax(Q @ K^T)\n",
    "    │   │   ├── Update KV cache for next iteration\n",
    "    │   │   └── output = attention_weights @ V\n",
    "    │   │\n",
    "    │   ├── Feed-forward network\n",
    "    │   │   ├── output = MLP(attention_output)\n",
    "    │   │   └── return + residual connection\n",
    "    │   │\n",
    "    │   └── Layer normalization & residuals\n",
    "    │\n",
    "    ├── Final layer norm\n",
    "    │\n",
    "    ├── Project to vocabulary\n",
    "    │   └── logits = output @ W_lm_head\n",
    "    │\n",
    "    └── return logits  # Scores for each token\n",
    "```\n",
    "\n",
    "**Key Insight: KV Caching**\n",
    "- Instead of recomputing attention for all previous tokens, we cache their K,V\n",
    "- Only compute attention for the new token against cached values\n",
    "- This is the main speedup in autoregressive generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e3265",
   "metadata": {},
   "source": [
    "## Layer 11: Sampling\n",
    "\n",
    "### File: `vllm/model_executor/layers/sampler.py`\n",
    "\n",
    "**After the model produces logits, we need to sample the next token**\n",
    "\n",
    "```python\n",
    "class Sampler:\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        logits: torch.Tensor,          # [batch_size, vocab_size]\n",
    "        sampling_params: SamplingParams,\n",
    "        ...\n",
    "    ) -> SamplerOutput:\n",
    "        \n",
    "        ├── 1. Apply Logits Processors (custom constraints)\n",
    "        │   └── logits = apply_processors(logits)\n",
    "        │\n",
    "        ├── 2. Temperature Scaling\n",
    "        │   └── scaled_logits = logits / temperature\n",
    "        │\n",
    "        ├── 3. Frequency Penalty\n",
    "        │   └── Reduce scores for repeated tokens\n",
    "        │\n",
    "        ├── 4. Presence Penalty\n",
    "        │   └── Reduce scores for tokens that appeared\n",
    "        │\n",
    "        ├── 5. Top-K Filtering\n",
    "        │   └── Keep only top-k highest probability tokens\n",
    "        │\n",
    "        ├── 6. Top-P (Nucleus) Sampling\n",
    "        │   └── Keep tokens until cumulative probability ≥ p\n",
    "        │\n",
    "        ├── 7. Convert Logits to Probabilities\n",
    "        │   └── probs = softmax(logits)\n",
    "        │\n",
    "        ├── 8. Sample Token ID\n",
    "        │   ├── Greedy: argmax(probs)\n",
    "        │   ├── Stochastic: multinomial(probs)\n",
    "        │   └── Beam search: keep top-b tokens\n",
    "        │\n",
    "        ├── 9. Compute Log Probabilities (if requested)\n",
    "        │   └── log_probs = log(probs)\n",
    "        │\n",
    "        └── return SamplerOutput\n",
    "           ├── sampled_token_ids\n",
    "           ├── logprobs\n",
    "           └── other metadata\n",
    "```\n",
    "\n",
    "**Sampling Parameters Control:**\n",
    "- `temperature` - Randomness (0=greedy, >1=more random)\n",
    "- `top_k` - Keep top-k tokens\n",
    "- `top_p` - Nucleus sampling parameter\n",
    "- `frequency_penalty`, `presence_penalty` - Repetition control\n",
    "- `use_beam_search` - Enable beam search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81cfbf",
   "metadata": {},
   "source": [
    "## Layer 12: Output Processing\n",
    "\n",
    "### Files: `vllm/engine/llm_engine.py` (Method: `_process_model_outputs()`)\n",
    "\n",
    "Back in the engine, after getting sampler output:\n",
    "\n",
    "```python\n",
    "def _process_model_outputs(ctx: SchedulerContext):\n",
    "    \n",
    "    ├── For each SamplerOutput:\n",
    "    │\n",
    "    ├── Decode sampled tokens to text\n",
    "    │   ├── Get token IDs from sampler output\n",
    "    │   ├── Convert to token strings\n",
    "    │   └── output_text = detokenizer.decode(tokens)\n",
    "    │\n",
    "    ├── Update Sequences with new tokens\n",
    "    │   ├── sequence.append_token_id(token_id)\n",
    "    │   ├── sequence.append_output_token(token, logprob)\n",
    "    │   └── Update sequence length\n",
    "    │\n",
    "    ├── Check Stop Conditions\n",
    "    │   ├── Reached max_tokens?\n",
    "    │   ├── Generated stop_token?\n",
    "    │   ├── Stop string detected?\n",
    "    │   └── If yes: mark sequence as finished\n",
    "    │\n",
    "    ├── Process Logprobs (if requested)\n",
    "    │   ├── Return top-k token probabilities\n",
    "    │   └── Track log probabilities for analysis\n",
    "    │\n",
    "    ├── Create RequestOutput objects\n",
    "    │   ├── request_id\n",
    "    │   ├── prompt (original input)\n",
    "    │   ├── outputs (list of CompletionOutput)\n",
    "    │   │   ├── text (generated text)\n",
    "    │   │   ├── finish_reason (stop, length, etc.)\n",
    "    │   │   ├── cumulative_logprob\n",
    "    │   │   └── logprobs\n",
    "    │   ├── prompt_token_ids\n",
    "    │   ├── finished (boolean)\n",
    "    │   └── metadata\n",
    "    │\n",
    "    └── return List[RequestOutput]\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42198f79",
   "metadata": {},
   "source": [
    "## Layer 13: Response Formatting\n",
    "\n",
    "### File: `vllm/entrypoints/openai/serving_chat.py` (continues after engine calls)\n",
    "\n",
    "After outputs are received from the engine, they're formatted as OpenAI API responses:\n",
    "\n",
    "```python\n",
    "# In create_chat_completion():\n",
    "\n",
    "async for request_output in results_generator:\n",
    "    ├── Extract completion outputs\n",
    "    ├── Format as OpenAI ChatCompletion format:\n",
    "    │\n",
    "    ├── ChatCompletionResponse:\n",
    "    │   ├── id - unique response ID\n",
    "    │   ├── object - \"text_completion\"\n",
    "    │   ├── created - timestamp\n",
    "    │   ├── model - model name\n",
    "    │   ├── choices - list of choices:\n",
    "    │   │   ├── message - ChatMessage\n",
    "    │   │   │   ├── role - \"assistant\"\n",
    "    │   │   │   └── content - generated text\n",
    "    │   │   ├── finish_reason - \"stop\", \"length\", etc.\n",
    "    │   │   └── index - choice index\n",
    "    │   ├── usage - UsageInfo\n",
    "    │   │   ├── prompt_tokens - input token count\n",
    "    │   │   ├── completion_tokens - output token count\n",
    "    │   │   └── total_tokens - sum\n",
    "    │   └── system_fingerprint - for reproducibility\n",
    "    │\n",
    "    └── For streaming: yield formatted chunks\n",
    "       └── ChatCompletionStreamResponse\n",
    "          └── Contains delta (incremental content)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a7ddc",
   "metadata": {},
   "source": [
    "## Complete Code Flow Diagram\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    HTTP REST API Request                                 │\n",
    "│                 POST /v1/chat/completions                                │\n",
    "│              (JSON with messages, model, parameters)                      │\n",
    "└──────────────────────────────┬──────────────────────────────────────────┘\n",
    "                               │\n",
    "                 ┌─────────────▼─────────────┐\n",
    "                 │   FastAPI Endpoint        │\n",
    "                 │  api_server.py            │\n",
    "                 └─────────────┬─────────────┘\n",
    "                               │\n",
    "                 ┌─────────────▼──────────────────┐\n",
    "                 │  OpenAIServingChat             │\n",
    "                 │  serving_chat.py               │\n",
    "                 │  - Validate request            │\n",
    "                 │  - Preprocess messages        │\n",
    "                 │  - Apply chat template        │\n",
    "                 └─────────────┬──────────────────┘\n",
    "                               │\n",
    "                 ┌─────────────▼──────────────────┐\n",
    "                 │  AsyncLLMEngine               │\n",
    "                 │  async_llm_engine.py          │\n",
    "                 │  - add_request()              │\n",
    "                 │  - Queue for processing       │\n",
    "                 └─────────────┬──────────────────┘\n",
    "                               │\n",
    "            ┌──────────────────┴───────────────────────┐\n",
    "            │    Background Engine Loop (Continuous)   │\n",
    "            │           run_engine_loop()              │\n",
    "            └──────────────────┬───────────────────────┘\n",
    "                               │\n",
    "        ┌──────────────────────▼──────────────────────┐\n",
    "        │  LLMEngine.step()                          │\n",
    "        │  - Scheduler: Select sequences             │\n",
    "        │  - Executor: Run model                     │\n",
    "        │  - Post-process: Decode & update          │\n",
    "        │  - Return outputs                          │\n",
    "        └──────┬─────────────────────────┬───────────┘\n",
    "               │                         │\n",
    "       ┌───────▼──────┐         ┌───────▼──────────┐\n",
    "       │  Scheduler   │         │  ModelExecutor   │\n",
    "       │              │         │                  │\n",
    "       │ - Batch seqs │         │ - Distribute work│\n",
    "       │ - KV cache   │         │ - Call workers   │\n",
    "       │   management │         │                  │\n",
    "       └──────────────┘         └────────┬─────────┘\n",
    "                                         │\n",
    "                              ┌──────────▼────────────┐\n",
    "                              │  Worker               │\n",
    "                              │  worker_base.py      │\n",
    "                              │                       │\n",
    "                              │ - Prepare inputs     │\n",
    "                              │ - KV cache ops       │\n",
    "                              │ - Call model_runner  │\n",
    "                              └──────────┬───────────┘\n",
    "                                         │\n",
    "                              ┌──────────▼─────────────┐\n",
    "                              │  ModelRunner          │\n",
    "                              │  model_runner.py      │\n",
    "                              │                        │\n",
    "                              │ - Tokenize inputs     │\n",
    "                              │ - Setup attention     │\n",
    "                              │ - Call model.forward()│\n",
    "                              │ - Sampler: pick token │\n",
    "                              └──────────┬────────────┘\n",
    "                                         │\n",
    "                              ┌──────────▼───────────────┐\n",
    "                              │  LLM Model              │\n",
    "                              │  Transformer            │\n",
    "                              │                          │\n",
    "                              │ - Embeddings            │\n",
    "                              │ - Attention layers      │\n",
    "                              │ - FFN layers            │\n",
    "                              │ - Output logits         │\n",
    "                              └──────────┬──────────────┘\n",
    "                                         │\n",
    "                              ┌──────────▼──────────┐\n",
    "                              │  Sampler            │\n",
    "                              │                      │\n",
    "                              │ - Temperature        │\n",
    "                              │ - Top-K              │\n",
    "                              │ - Top-P              │\n",
    "                              │ - Sample token ID    │\n",
    "                              └──────────┬──────────┘\n",
    "                                         │\n",
    "                              ┌──────────▼───────────────┐\n",
    "                              │  Output Processing       │\n",
    "                              │                          │\n",
    "                              │ - Detokenize            │\n",
    "                              │ - Check stop conditions  │\n",
    "                              │ - Create RequestOutput   │\n",
    "                              └──────────┬───────────────┘\n",
    "                                         │\n",
    "                ┌────────────────────────▼──────────────────┐\n",
    "                │  Response Formatting                      │\n",
    "                │  (Back in OpenAIServingChat)              │\n",
    "                │                                            │\n",
    "                │ - Convert to ChatCompletionResponse        │\n",
    "                │ - Add usage statistics                     │\n",
    "                │ - Handle streaming vs non-streaming       │\n",
    "                └────────────────┬───────────────────────────┘\n",
    "                                 │\n",
    "                ┌────────────────▼────────────────┐\n",
    "                │  HTTP Response (JSON)           │\n",
    "                │  Sent back to client             │\n",
    "                └─────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0168a4",
   "metadata": {},
   "source": [
    "## Key Data Structures\n",
    "\n",
    "### 1. Sequence & SequenceGroup\n",
    "\n",
    "```python\n",
    "Sequence:\n",
    "  ├── request_id - Links to original request\n",
    "  ├── prompt_token_ids - Tokenized input\n",
    "  ├── output_token_ids - Generated tokens so far\n",
    "  ├── logprobs - Log probabilities per token\n",
    "  ├── status - WAITING, RUNNING, FINISHED\n",
    "  ├── kv_cache - Cached key-value tensors\n",
    "  └── generation_config - Temperature, top_k, etc.\n",
    "\n",
    "SequenceGroup:\n",
    "  ├── request_id - Unique request identifier\n",
    "  ├── seqs - List[Sequence] (usually n=1, but more for beam search)\n",
    "  ├── arrival_time - When request arrived\n",
    "  ├── sampling_params - SamplingParams\n",
    "  ├── lora_request - LoRA adapter (optional)\n",
    "  └── status - PENDING, RUNNING, FINISHED\n",
    "```\n",
    "\n",
    "### 2. SamplingParams\n",
    "\n",
    "```python\n",
    "SamplingParams:\n",
    "  ├── temperature - Randomness (default: 1.0)\n",
    "  ├── top_p - Nucleus sampling (default: 1.0)\n",
    "  ├── top_k - Top-k filtering (default: -1, disabled)\n",
    "  ├── max_tokens - Maximum length (default: None)\n",
    "  ├── frequency_penalty - Penalize repeated tokens\n",
    "  ├── presence_penalty - Penalize seen tokens\n",
    "  ├── use_beam_search - Enable beam search\n",
    "  ├── best_of - Number of beams\n",
    "  ├── repetition_penalty - Another repetition control\n",
    "  ├── stop - Stop strings/tokens\n",
    "  ├── logprobs - Number of logprobs to return\n",
    "  └── prompt_logprobs - Include prompt logprobs\n",
    "```\n",
    "\n",
    "### 3. RequestOutput\n",
    "\n",
    "```python\n",
    "RequestOutput:\n",
    "  ├── request_id - Matches the request\n",
    "  ├── prompt - Original prompt text\n",
    "  ├── prompt_token_ids - Tokenized input\n",
    "  ├── prompt_logprobs - Token probabilities from prompt (optional)\n",
    "  ├── outputs - List[CompletionOutput]\n",
    "  │   ├── text - Generated text\n",
    "  │   ├── finish_reason - \"stop\", \"length\", etc.\n",
    "  │   ├── cumulative_logprob - Sum of log probabilities\n",
    "  │   └── logprobs - Per-token probabilities\n",
    "  ├── finished - Whether generation is complete\n",
    "  └── metadata - Request timing and stats\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c70559",
   "metadata": {},
   "source": [
    "## Important Concepts\n",
    "\n",
    "### KV Cache (Key-Value Cache)\n",
    "\n",
    "**Why it matters:** The main performance optimization in LLM serving\n",
    "\n",
    "In standard transformer attention:\n",
    "```\n",
    "For each token position i:\n",
    "  - Compute Query(i) from current token\n",
    "  - Compute attention against ALL previous K,V values\n",
    "  - This is O(n²) with sequence length\n",
    "```\n",
    "\n",
    "With KV caching:\n",
    "```\n",
    "On first pass (prefill):\n",
    "  - Compute all Q,K,V\n",
    "  - Store K,V for reuse\n",
    "  \n",
    "On subsequent passes (decode):\n",
    "  - Only compute Q for new token\n",
    "  - Reuse cached K,V from previous tokens\n",
    "  - This is O(n) with sequence length\n",
    "  \n",
    "Result: ~100x speedup for long sequences\n",
    "```\n",
    "\n",
    "**vLLM manages KV cache:**\n",
    "- Allocates blocks of memory\n",
    "- Tracks which blocks are in use\n",
    "- Swaps blocks between GPU/CPU\n",
    "- Copies blocks for beam search\n",
    "\n",
    "### Batching Strategy\n",
    "\n",
    "vLLM uses **continuous batching** (also called dynamic batching):\n",
    "\n",
    "```\n",
    "Traditional batching:\n",
    "  - Wait for full batch before running\n",
    "  - All sequences must finish together\n",
    "  - Wasted GPU time if some finish early\n",
    "\n",
    "Continuous batching (vLLM):\n",
    "  - Add new requests anytime\n",
    "  - Remove finished sequences\n",
    "  - Pack GPU with running sequences\n",
    "  - Much higher utilization\n",
    "```\n",
    "\n",
    "This is done by the Scheduler in `engine.step()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7320db4",
   "metadata": {},
   "source": [
    "## File Structure Summary\n",
    "\n",
    "```\n",
    "vllm/\n",
    "├── entrypoints/\n",
    "│   ├── openai/\n",
    "│   │   ├── api_server.py              [Layer 1] HTTP entry point\n",
    "│   │   ├── serving_chat.py            [Layer 2] Chat request handling\n",
    "│   │   ├── serving_engine.py          [Layer 3] Base serving logic\n",
    "│   │   └── protocol.py                Request/response schemas\n",
    "│   ├── llm.py                         High-level LLM class\n",
    "│   └── utils.py                       Utility functions\n",
    "│\n",
    "├── engine/\n",
    "│   ├── llm_engine.py                  [Layer 5] Core engine\n",
    "│   ├── async_llm_engine.py            [Layer 4] Async wrapper\n",
    "│   ├── protocol.py                    Engine interface\n",
    "│   └── ...\n",
    "│\n",
    "├── core/\n",
    "│   └── scheduler.py                   [Layer 6] Scheduling logic\n",
    "│\n",
    "├── executor/\n",
    "│   ├── executor_base.py               [Layer 7] Base executor\n",
    "│   ├── uniproc_executor.py            Single GPU executor\n",
    "│   └── ...\n",
    "│\n",
    "├── worker/\n",
    "│   ├── worker_base.py                 [Layer 8] Worker interface\n",
    "│   ├── model_runner.py                [Layer 9] Model execution\n",
    "│   └── ...\n",
    "│\n",
    "├── model_executor/\n",
    "│   ├── models/                        [Layer 10] Model implementations\n",
    "│   │   ├── llama.py\n",
    "│   │   ├── mistral.py\n",
    "│   │   └── ...\n",
    "│   └── layers/\n",
    "│       ├── sampler.py                 [Layer 11] Token sampling\n",
    "│       ├── attention.py\n",
    "│       └── ...\n",
    "│\n",
    "├── config.py                          Configuration classes\n",
    "├── sampling_params.py                 Sampling parameters\n",
    "├── sequence.py                        Sequence/SequenceGroup\n",
    "└── outputs.py                         Output classes\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314e97e",
   "metadata": {},
   "source": [
    "## Execution Timeline for a Single Request\n",
    "\n",
    "### Time 0: Request Arrives\n",
    "\n",
    "```python\n",
    "POST /v1/chat/completions\n",
    "{\n",
    "  \"model\": \"meta-llama/Llama-2-7b\",\n",
    "  \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n",
    "  \"temperature\": 0.7,\n",
    "  \"max_tokens\": 100\n",
    "}\n",
    "```\n",
    "\n",
    "### Time 1: Request Processing (FastAPI Handler)\n",
    "\n",
    "- Parse JSON\n",
    "- Validate request format\n",
    "- Pass to OpenAIServingChat.create_chat_completion()\n",
    "\n",
    "### Time 2: Preprocessing (OpenAIServingChat)\n",
    "\n",
    "- Get tokenizer\n",
    "- Apply chat template\n",
    "- Convert to prompt format: `\"Hello, how are you?\"`\n",
    "- Tokenize: `[13881, 29892, 1920, 526, 366, 29973]`\n",
    "- Create SamplingParams(temperature=0.7, max_tokens=100)\n",
    "\n",
    "### Time 3: Queue Request (AsyncLLMEngine)\n",
    "\n",
    "- Generate request_id: e.g., `\"req-001\"`\n",
    "- Create AsyncStream for tracking output\n",
    "- Call `add_request()` on background engine\n",
    "- Immediately return async generator\n",
    "\n",
    "### Time 4: Scheduler Picks Request (Engine.step())\n",
    "\n",
    "**Iteration 1:**\n",
    "- Scheduler sees new request in queue\n",
    "- Checks: Can it fit on GPU?\n",
    "- Allocates: KV cache blocks, attention memory\n",
    "- Creates SequenceGroupMetadata\n",
    "- Marks as ready to run\n",
    "\n",
    "### Time 5: Prefill Pass (Tokenization → Attention)\n",
    "\n",
    "```python\n",
    "ModelRunner.execute_model()\n",
    "  ├── input_ids = [13881, 29892, 1920, 526, 366, 29973]\n",
    "  ├── Model forward pass\n",
    "  │   ├── Embed all input tokens\n",
    "  │   ├── Self-attention over ALL input tokens\n",
    "  │   ├── Store full KV cache\n",
    "  │   └── Output logits for last position\n",
    "  ├── Sampler picks next token (e.g., ID=29892 for \",\")\n",
    "  └── Append to sequence\n",
    "```\n",
    "\n",
    "**Result:** sequence_tokens = `[13881, 29892, 1920, 526, 366, 29973, 29892]`\n",
    "\n",
    "### Time 6-N: Decode Loop (Repeated)\n",
    "\n",
    "**For each new token (iterations 2-100):**\n",
    "\n",
    "```python\n",
    "Iteration 2:\n",
    "  ├── input_ids = [29892]  ← Only new token!\n",
    "  ├── Model forward pass\n",
    "  │   ├── Embed new token\n",
    "  │   ├── Attention: Query against CACHED K,V\n",
    "  │   ├── Update KV cache with new K,V\n",
    "  │   └── Output logits for this position\n",
    "  ├── Sampler picks next token\n",
    "  └── Append to sequence\n",
    "```\n",
    "\n",
    "**Performance:** ~100x faster than prefill due to KV cache\n",
    "\n",
    "### Time N+1: Stop Condition Met\n",
    "\n",
    "```python\n",
    "- Generated \"<|endoftext|>\" token OR\n",
    "- Reached max_tokens=100 OR\n",
    "- Stop string found\n",
    "└── Mark sequence as FINISHED\n",
    "```\n",
    "\n",
    "### Time N+2: Post-Processing & Response\n",
    "\n",
    "```python\n",
    "- Detokenize: [13881, 29892, ...] → \"Hello, I'm doing great!\"\n",
    "- Create RequestOutput\n",
    "- Format as ChatCompletionResponse\n",
    "- Return to client\n",
    "\n",
    "Response:\n",
    "{\n",
    "  \"id\": \"chatcmpl-001\",\n",
    "  \"object\": \"chat.completion\",\n",
    "  \"created\": 1234567890,\n",
    "  \"model\": \"meta-llama/Llama-2-7b\",\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Hello, I'm doing great! How are you?\"\n",
    "      },\n",
    "      \"finish_reason\": \"stop\"\n",
    "    }\n",
    "  ],\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 6,\n",
    "    \"completion_tokens\": 12,\n",
    "    \"total_tokens\": 18\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7c83e",
   "metadata": {},
   "source": [
    "## Performance Optimizations\n",
    "\n",
    "### 1. Prefill vs Decode\n",
    "\n",
    "| Phase | Input | Compute | KV Cache | Speed |\n",
    "|-------|-------|---------|----------|-------|\n",
    "| **Prefill** | Full prompt | Attention over all tokens | Build full cache | Slower (memory-bound) |\n",
    "| **Decode** | 1 token | Attention to 1 token | Use cached KV | Faster (compute-bound) |\n",
    "\n",
    "### 2. Continuous Batching\n",
    "\n",
    "```python\n",
    "# Without continuous batching:\n",
    "Batch 1: [Req1, Req2, Req3] → Wait for all to finish\n",
    "Batch 2: [Req4, Req5] → Then run these\n",
    "\n",
    "# With continuous batching (vLLM):\n",
    "Iteration 1: [Req1-prefill, Req2-prefill, Req3-prefill, Req4-decode]\n",
    "Iteration 2: [Req2-prefill, Req3-decode, Req4-decode, Req5-prefill]\n",
    "Iteration 3: [Req3-decode, Req4-decode, Req5-decode, Req1-decode]\n",
    "→ Constant GPU utilization!\n",
    "```\n",
    "\n",
    "### 3. Block-wise KV Cache Management\n",
    "\n",
    "```\n",
    "GPU KV Cache:\n",
    "┌─────────────────────────┐\n",
    "│ Block 0: Req1 tokens    │\n",
    "├─────────────────────────┤\n",
    "│ Block 1: Req2 tokens    │\n",
    "├─────────────────────────┤\n",
    "│ Block 2: Req3 tokens    │\n",
    "├─────────────────────────┤\n",
    "│ Block 3: Req4 tokens    │\n",
    "├─────────────────────────┤\n",
    "│ Block 4: FREE           │\n",
    "└─────────────────────────┘\n",
    "\n",
    "- Allocate blocks on demand\n",
    "- Swap to CPU when GPU full\n",
    "- Copy blocks for beam search\n",
    "```\n",
    "\n",
    "### 4. Flash Attention\n",
    "\n",
    "- **Standard:** Attention is O(n²) memory (materialized attention matrix)\n",
    "- **Flash Attention:** Compute attention tiles, reduce I/O\n",
    "- Result: ~2-4x speedup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa53936",
   "metadata": {},
   "source": [
    "## Quick Reference: Finding Code\n",
    "\n",
    "### To understand each layer:\n",
    "\n",
    "| Layer | Key File | Key Class/Function | Line | Purpose |\n",
    "|-------|----------|-------------------|------|---------|\n",
    "| 1. HTTP API | `api_server.py` | FastAPI routes | - | Accept HTTP requests |\n",
    "| 2. Chat Serving | `serving_chat.py` | `OpenAIServingChat.create_chat_completion()` | ~200 | Parse and preprocess chat |\n",
    "| 3. Serving Logic | `serving_engine.py` | `OpenAIServing._generate_completions()` | ~150 | Interface to engine |\n",
    "| 4. Async Engine | `async_llm_engine.py` | `AsyncLLMEngine.add_request()`, `.generate()` | - | Async wrapper |\n",
    "| 5. Core Engine | `llm_engine.py` | `LLMEngine.step()` | 1194 | Main inference loop |\n",
    "| 6. Scheduling | `scheduler.py` | `Scheduler.schedule()` | - | Batch scheduling |\n",
    "| 7. Executor | `executor_base.py` | `ExecutorBase.execute_model()` | 143 | Executor abstraction |\n",
    "| 8. Worker | `worker_base.py` | `LocalOrDistributedWorkerBase.execute_model()` | 385 | Actual worker |\n",
    "| 9. Model Runner | `model_runner.py` | `ModelRunner.execute_model()` | 234 | Prepare and run model |\n",
    "| 10. Model Forward | `llama.py` (etc) | `LlamaForCausalLM.forward()` | - | Transformer logic |\n",
    "| 11. Sampling | `sampler.py` | `Sampler.forward()` | - | Token selection |\n",
    "| 12. Output Processing | `llm_engine.py` | `LLMEngine._process_model_outputs()` | - | Decode & finalize |\n",
    "| 13. Response | `serving_chat.py` | Format response | - | Format as JSON |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6aa858",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### The vLLM Request Journey\n",
    "\n",
    "1. **API Request Arrives** → FastAPI handler in `api_server.py`\n",
    "2. **Request Validated & Preprocessed** → `serving_chat.py` applies chat template\n",
    "3. **Queued for Processing** → `async_llm_engine.py` adds to request tracker\n",
    "4. **Scheduled in Engine Loop** → `llm_engine.py.step()` runs continuously\n",
    "5. **Sequences Batched** → `scheduler.py` decides which sequences to execute\n",
    "6. **Model Executed** → Passed through `executor.py` → `worker.py` → `model_runner.py`\n",
    "7. **Transformer Forward Pass** → Model computes logits using attention & FFN\n",
    "8. **Token Sampled** → `sampler.py` picks next token with temperature/top-k\n",
    "9. **Sequence Updated** → Token appended, KV cache updated\n",
    "10. **Loop Repeats** → Steps 4-9 until stop condition\n",
    "11. **Output Post-Processed** → Tokens detokenized to text\n",
    "12. **Response Formatted** → Converted to OpenAI API format\n",
    "13. **Response Sent** → HTTP response returned to client\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The entire system is built on:\n",
    "- **Continuous batching** - Maximize GPU utilization\n",
    "- **KV caching** - 100x speedup for decode phase\n",
    "- **Block-wise cache management** - Enable swapping and sharing\n",
    "- **Asynchronous I/O** - Handle multiple requests simultaneously\n",
    "- **Modular architecture** - Support multiple hardware backends\n",
    "\n",
    "This is why vLLM achieves 10-100x throughput improvements over standard inference.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
